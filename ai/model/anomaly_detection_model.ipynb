{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24a622d3-7c72-4ff5-b66a-3c0ca61feb8e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 사용 가능\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available() == True:\n",
    "    device = 'cuda:0'\n",
    "    print('GPU 사용 가능')\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print('GPU 사용 불가')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa741ed3-0fd4-47c6-b3e3-a3f6ba956bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# 학습 추이 기록\n",
    "def init_log():\n",
    "    plt.rc('font', size = 10)\n",
    "    # 모든 log를 초기화\n",
    "    global log_stack, iter_log, tloss_log, vloss_log, time_log\n",
    "    iter_log, tloss_log, vloss_log = [], [], []\n",
    "    time_log, log_stack = [], []\n",
    "\n",
    "def record_train_log(_tloss, _time):\n",
    "    # Train Log 기록용\n",
    "    time_log.append(_time)\n",
    "    tloss_log.append(_tloss)\n",
    "    iter_log.append(epoch_cnt)\n",
    "\n",
    "def record_valid_log(_vloss):\n",
    "    # Validation Log 기록용\n",
    "    vloss_log.append(_vloss)\n",
    "\n",
    "def last(log_list):\n",
    "    # 리스트 안의 마지막 숫자를 반환(print_log 함수에서 사용)\n",
    "    if len(log_list) > 0:\n",
    "        return log_list[len(log_list) - 1]\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "from IPython.display import clear_output\n",
    "def print_log():\n",
    "    # 학습 추이 출력\n",
    "\n",
    "    # 소숫점 3자리 수까지 조절\n",
    "    train_loss = round(float(last(tloss_log)), 3)\n",
    "    val_loss = round(float(last(vloss_log)), 3)\n",
    "    time_spent = round(float(last(time_log)), 3)\n",
    "\n",
    "    log_str = f'Epoch: {last(iter_log):3} | T_Loss {train_loss:5} | V_Loss {val_loss:5} | Time {time_spent:5}'\n",
    "\n",
    "    log_stack.append(log_str)\n",
    "\n",
    "    # 학습 추이 그래프 출력\n",
    "    hist_fig, loss_axis = plt.subplots(figsize=(10, 3), dpi=99) # 그래프 사이즈 설정\n",
    "    hist_fig.patch.set_facecolor('white') # 그래프 배경색 설정\n",
    "    \n",
    "    # Loss Line 구성\n",
    "    loss_t_line = plt.plot(iter_log, tloss_log, label='Train Loss', color='red', marker='o')\n",
    "    loss_v_line = plt.plot(iter_log, vloss_log, label='Valid Loss', color='blue', marker='s')\n",
    "    loss_axis.set_xlabel('epoch')\n",
    "    loss_axis.set_ylabel('loss')\n",
    "    \n",
    "    # 그래프 출력\n",
    "    hist_lines = loss_t_line + loss_v_line # 위에서 선언한 plt정보들 통합\n",
    "    loss_axis.legend(hist_lines, [l.get_label() for l in hist_lines]) # 순서대로 그려주기\n",
    "    loss_axis.grid() # 격자 설정\n",
    "    plt.title('Learning history until epoch {}'.format(last(iter_log)))\n",
    "    plt.draw()\n",
    "    \n",
    "    # 텍스트 로그 출력\n",
    "    clear_output(wait=True)\n",
    "    plt.show()\n",
    "    for idx in reversed(range(len(log_stack))): # 반대로 sort 시켜서 출력\n",
    "        print(log_stack[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea300e37-0ddd-469c-8ef4-9cc722ec6638",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T08:32:52.401661Z",
     "start_time": "2024-05-27T08:32:52.380657Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8529\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# train dataset 집어넣기\n",
    "dataset = []\n",
    "\n",
    "def load_dataset(name):\n",
    "    forehand = np.load(f'../variable_dataset/training_data/{name}/{name}_B_Forehand_sampling_train_dataset.npy')\n",
    "    backhand = np.load(f'../variable_dataset/training_data/{name}/{name}_B_Backhand_sampling_train_dataset.npy')\n",
    "    backslice = np.load(f'../variable_dataset/training_data/{name}/{name}_B_BackSlice_sampling_train_dataset.npy')\n",
    "    forevolley = np.load(f'../variable_dataset/training_data/{name}/{name}_B_ForeVolley_sampling_train_dataset.npy')\n",
    "    backvolley = np.load(f'../variable_dataset/training_data/{name}/{name}_B_BackVolley_sampling_train_dataset.npy')\n",
    "    smash = np.load(f'../variable_dataset/training_data/{name}/{name}_B_Smash_sampling_train_dataset.npy')\n",
    "    serve = np.load(f'../variable_dataset/training_data/{name}/{name}_B_Serve_sampling_train_dataset.npy')\n",
    "\n",
    "    for i in forehand:\n",
    "        dataset.append(i)\n",
    "    for i in backhand:\n",
    "        dataset.append(i)\n",
    "    for i in backslice:\n",
    "        dataset.append(i)\n",
    "    for i in forevolley:\n",
    "        dataset.append(i)\n",
    "    for i in backvolley:\n",
    "        dataset.append(i)\n",
    "    for i in smash:\n",
    "        dataset.append(i)\n",
    "    for i in serve:\n",
    "        dataset.append(i)\n",
    "\n",
    "load_dataset('Federer')\n",
    "load_dataset('Nadal_inv')\n",
    "load_dataset('Djokovic')\n",
    "load_dataset('Sinner')\n",
    "load_dataset('Tsitsipas')\n",
    "load_dataset('Zverev')\n",
    "load_dataset('Murray')\n",
    "load_dataset('Alcaraz')\n",
    "load_dataset('Rune')\n",
    "load_dataset('Shapovalov_inv')\n",
    "\n",
    "random.shuffle(dataset)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad84ff91-dd0e-4202-8be7-e20eea2de81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader # Pytorch에서 데이터를 불러오고, 전처리하는 클래스\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, seq_data):\n",
    "        self.dataset = []\n",
    "        for data in seq_data:\n",
    "            self.dataset.append(data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.dataset[index]\n",
    "        return torch.Tensor(np.array(data))\n",
    "        \n",
    "    def __len__(self):\n",
    "            return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f8005ed-6739-4927-b8ef-a1810c782b16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T08:33:00.101893Z",
     "start_time": "2024-05-27T08:33:00.098892Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6823, 1706\n"
     ]
    }
   ],
   "source": [
    "split_ratio = [0.8, 0.2]\n",
    "train_len = round(len(dataset) * split_ratio[0])\n",
    "val_len = round(len(dataset) * split_ratio[1])\n",
    "print(f'{train_len}, {val_len}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "163e98ce-7bc7-4908-918c-d05b6745cf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "train_dataset = MyDataset(dataset)\n",
    "train_data, valid_data = random_split(train_dataset, [train_len, val_len])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16)\n",
    "val_loader = DataLoader(valid_data, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91dfbec9-4e17-41eb-86ed-b7eb026e1260",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size=100, hidden_size=50, num_layers=2):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True,\n",
    "                            dropout=0.3, bidirectional=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs, (hidden, cell) = self.lstm(x)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "\n",
    "        return (hidden, cell)\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size=100, hidden_size=50, output_size=100, num_layers=2):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True,\n",
    "                            dropout=0.3, bidirectional=False)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        output, (hidden, cell) = self.lstm(x, hidden)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        prediction = self.fc(output)\n",
    "\n",
    "        return prediction, (hidden, cell)\n",
    "    \n",
    "## LSTM Auto Encoder\n",
    "class LSTMAutoEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_dim: int,\n",
    "                 latent_dim: int,\n",
    "                 sequence_length: int=1,\n",
    "                 **kwargs) -> None:\n",
    "        \"\"\"\n",
    "        :param input_dim: 변수 Tag 갯수\n",
    "        :param latent_dim: 최종 압축할 차원 크기\n",
    "        :param sequence length: sequence 길이\n",
    "        :param kwargs:\n",
    "        \"\"\"\n",
    "\n",
    "        super(LSTMAutoEncoder, self).__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        if \"num_layers\" in kwargs:\n",
    "            num_layers = kwargs.pop(\"num_layers\")\n",
    "        else:\n",
    "            num_layers = 1\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=latent_dim,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        self.reconstruct_decoder = Decoder(\n",
    "            input_size=input_dim,\n",
    "            output_size=input_dim,\n",
    "            hidden_size=latent_dim,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "\n",
    "    def forward(self, src:torch.Tensor, **kwargs):\n",
    "        batch_size, sequence_length, var_length = src.size()\n",
    "\n",
    "        ## Encoder 넣기\n",
    "        encoder_hidden = self.encoder(src)\n",
    "        \n",
    "        inv_idx = torch.arange(sequence_length - 1, -1, -1).long()\n",
    "        reconstruct_output = []\n",
    "        temp_input = torch.zeros((batch_size, 1, var_length), dtype=torch.float).to(src.device)\n",
    "        hidden = encoder_hidden\n",
    "        for t in range(sequence_length):\n",
    "            temp_input, hidden = self.reconstruct_decoder(temp_input, hidden)\n",
    "            reconstruct_output.append(temp_input)\n",
    "        reconstruct_output = torch.cat(reconstruct_output, dim=1)[:, inv_idx, :]\n",
    "        \n",
    "        return [reconstruct_output, src]\n",
    "\n",
    "    def loss_function(self,\n",
    "                      *args,\n",
    "                      **kwargs) -> dict:\n",
    "        recons = args[0]\n",
    "        input = args[1]\n",
    "        \n",
    "        ## MSE loss(Mean squared Error)\n",
    "        loss =F.mse_loss(recons, input)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "daf3ba7e-b8c4-4370-89b7-7192d3dc1422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 초기화\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import Adam\n",
    "def init_model():\n",
    "    plt.rc('font', size = 10)\n",
    "    global model, loss_fn, optim\n",
    "    model = LSTMAutoEncoder(input_dim=100, latent_dim=50, sequence_length=60, num_layers=6).to(device)\n",
    "    optim = Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# epoch count 초기화\n",
    "def init_epoch():\n",
    "    global epoch_cnt\n",
    "    epoch_cnt = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "513fc3fd-28d1-49e7-b701-5253eeacbac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from torch.cuda import memory_allocated, empty_cache\n",
    "def clear_memory():\n",
    "    if device != 'cpu':\n",
    "        empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# 학습 알고리즘\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "def epoch(data_loader, mode = 'train'):\n",
    "    global epoch_cnt\n",
    "\n",
    "    # 사용되는 변수 초기화\n",
    "    iter_loss, last_grad_performed = [], False\n",
    "\n",
    "    # 1 iteration 학습 알고리즘(for 문을 나오면 1 epoch 완료)\n",
    "    for _data in tqdm(data_loader, desc='Training'):\n",
    "        data = _data.to(device)\n",
    "\n",
    "        # 1. Feed-forward\n",
    "        if mode == 'train':\n",
    "            model.train()\n",
    "        else:\n",
    "            # 학습때만 쓰이는 Dropout, Batch Mormalization을 미사용\n",
    "            model.eval()\n",
    "    \n",
    "        result = model(data) # 1 Batch에 대한 결과가 모든 Class에 대한 확률값으로 \n",
    "        \n",
    "        # 2. Loss 계산\n",
    "        loss = model.loss_function(*result) # GT 와 Label 비교하여 Loss 산정\n",
    "        iter_loss.append(loss.item()) # 학습 추이를 위하여 Loss를 기록\n",
    "    \n",
    "        # 3. 역전파 학습 후 Gradient Descent\n",
    "        if mode == 'train':\n",
    "            optim.zero_grad() # 미분을 통해 얻은 기울기로 초기화 for 다음 epoch\n",
    "            loss.backward() # 역전파 학습\n",
    "            optim.step() # Gradient Descent 수행\n",
    "            last_grad_performed = True # for문 나가면 epoch 카운터 += 1\n",
    "\n",
    "    if last_grad_performed:\n",
    "        epoch_cnt += 1\n",
    "\n",
    "    clear_memory()\n",
    "\n",
    "    # loss와 acc의 평균값 for 학습 추이 그래프, 모든 GT와 Label 값 for 컨퓨전 행렬\n",
    "    return np.average(iter_loss)\n",
    "\n",
    "def epoch_not_finished():\n",
    "    # 에폭이 끝남을 알림\n",
    "    return epoch_cnt < maximum_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0692bd7-c20b-4876-b001-3d4e573a5cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Initialization\n",
    "init_model()\n",
    "init_epoch()\n",
    "init_log()\n",
    "maximum_epoch = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32fc34d-3c3a-4308-a583-bf78364f9cbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|█████▋                                                   | 53/534 [00:03<00:31, 15.51it/s]"
     ]
    }
   ],
   "source": [
    "# Training Iteration\n",
    "import time\n",
    "loss_result = []\n",
    "while epoch_not_finished():\n",
    "    start_time = time.time()\n",
    "    tloss = epoch(train_loader, mode = 'train')\n",
    "    end_time = time.time()\n",
    "    time_taken = end_time - start_time\n",
    "    record_train_log(tloss, time_taken)\n",
    "    with torch.no_grad():\n",
    "        vloss = epoch(val_loader, mode = 'val')\n",
    "        record_valid_log(vloss)\n",
    "    print_log()\n",
    "    loss_result.append(f'tloss : {tloss:.5} | vloss : {vloss:.5}')\n",
    "    if tloss < 0.001 and vloss < 0.001:\n",
    "        torch.save(model.state_dict(), f'./model_info/anomaly_detection_model_{tloss:.5}_{vloss:.5}')\n",
    "    if epoch_cnt == 1500:\n",
    "        torch.save(model.state_dict(), f'./model_info/anomaly_detection_model_{tloss:.5}_{vloss:.5}_layer6_1500')\n",
    "\n",
    "print('\\n Training completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026b0994-8d6a-4382-bff3-443f7662be1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장하기\n",
    "torch.save(model.state_dict(), f'./model_info/anomaly_detection_model_{tloss:.5}_{vloss:.5}_layer6_2000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a75ad2-ed66-4e1e-b5e3-acf80008cc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "with open(\"anomaly_detection_train_result_layer6.txt\", \"w\") as f:\n",
    "    for i in loss_result:\n",
    "        cnt += 1\n",
    "        f.write(str(cnt))\n",
    "        f.write(\" \")\n",
    "        f.write(str(i))\n",
    "        f.write(\" \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa44f0b7-96f9-4fca-a8e8-a6825650d3ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
